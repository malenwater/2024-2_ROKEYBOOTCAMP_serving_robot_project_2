import numpy as np
import cv2
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CompressedImage, CameraInfo
from cv_bridge import CvBridge
from geometry_msgs.msg import TransformStamped
from tf2_msgs.msg import TFMessage  
from nav_msgs.msg import Odometry
from scipy.spatial.transform import Rotation as R
from geometry_msgs.msg import PoseWithCovarianceStamped
import numpy as np
from visualization_msgs.msg import Marker
import threading
from geometry_msgs.msg import Point
BASELINK_TO_CAMERA = np.array([ 
    [0.000, 0.000, 1.000, -0.060],
    [-1.000, 0.000, 0.000, 0.000],
    [0.000, -1.000, 0.000, 0.244],
    [0.000, 0.000, 0.000, 1.000]
])
  
# CAMERA_K =np.array([[202.39749146,   0.,         125.49773407],
#             [  0.,         202.39749146, 125.75233459],
#             [  0.,           0.,           1.        ]]) 
# CAMERA_D = np.array([[-3.51905060e+00, -2.84767342e+01, -3.02788394e-04,  1.01520610e-03,
#         2.35221481e+02, -3.68542147e+00, -2.67263298e+01,  2.28351166e+02]]) 


   
class SIFTDetector():
    def __init__(self, ori_img_ext,ori_img_man, types: int, CAMERA_K, CAMERA_D):
        self.ori_img_ext = ori_img_ext
        self.ori_img_man = ori_img_man
        self.cap_img = None
        self.result = False
        self.result_img = None
        self.types = types
        self.EXT = 0.18
        self.EXT_PIXEL = 680
        
        self.MAN_HEIGHT = 0.18
        self.MAN_WIGHT = 0.23
        self.MAN_HEIGHT_PIXEL = 680
        self.MAN_WIGHT_PIXEL = 869
        self.sift = cv2.SIFT_create()
        
        # print(ori_img.shape)
        # print(cap_img.shape)
        
        self.kp1_EXT, self.des1_EXT = self.sift.detectAndCompute(self.ori_img_ext, None)
        self.kp1_MAN, self.des1_MAN = self.sift.detectAndCompute(self.ori_img_man, None)
        self.CAMERA_K = CAMERA_K
        self.CAMERA_D = CAMERA_D
        # ëª¨ë“  ptì— ëŒ€í•´ ë³€í™˜ëœ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
        self.transformed_pts_EXT = [(kp.pt[0] / self.EXT_PIXEL * self.EXT, kp.pt[1] / self.EXT_PIXEL * self.EXT, 0) for kp in self.kp1_EXT]
        self.transformed_pts_MAN = [(kp.pt[0] / self.MAN_WIGHT_PIXEL * self.MAN_WIGHT, kp.pt[1] / self.MAN_HEIGHT_PIXEL * self.MAN_HEIGHT, 0) for kp in self.kp1_MAN]
        self.transformed_pts = None
        # print(f'self.cap_img.shape : {self.cap_img.shape}')
        self.max_dist = 0.5
        # ë³€í™˜ëœ pt ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
        # print(self.transformed_pts)
        # print(len(self.transformed_pts))
        # print()
        # print(self.des1.shape)
    def set_KD(self,K,D):
        self.CAMERA_K = K
        self.CAMERA_D = D
    def detect(self,cam_image):
        self.cap_img = cam_image
        if self.CAMERA_K is None or self.cap_img is None:
            return
        self.kp2, self.des2 = self.sift.detectAndCompute(self.cap_img, None)

        if self.des1_MAN is None or self.des2 is None or len(self.kp1_MAN) < 2 or len(self.kp2) < 2:
            return

        index_params = dict(algorithm=1, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches_EXT = flann.knnMatch(self.des1_EXT, self.des2, k=2)
        matches_MAN = flann.knnMatch(self.des1_MAN, self.des2, k=2)
        good_matches_EXT = [m for m, n in matches_EXT if m.distance < self.max_dist * n.distance]
        good_matches_MAN = [m for m, n in matches_MAN if m.distance < self.max_dist * n.distance]
        if len(good_matches_EXT) > len(good_matches_MAN):
            matches = matches_EXT
            self.transformed_pts = self.transformed_pts_EXT
        else:
            matches = matches_MAN
            self.transformed_pts = self.transformed_pts_MAN
            
        # print(matches)
        # print(len(matches))
        # good_matchesì—ì„œ ë§¤ì¹­ëœ ê° íŠ¹ì§•ì ì— ëŒ€í•´ transformed_ptsì™€ kp2ì˜ ì¢Œí‘œ ì§ì§€ê¸°
        object_points = []
        good_matches = []
        image_points = []
        for match, n in matches:
            # matchì˜ ì²« ë²ˆì§¸ ìš”ì†Œ(m)ëŠ” self.kp1ì—ì„œ, ë‘ ë²ˆì§¸ ìš”ì†Œ(n)ëŠ” self.kp2ì—ì„œ ë§¤ì¹­ëœ íŠ¹ì§•ì 
            # self.transformed_ptsëŠ” self.kp1ì—ì„œ ì¶”ì¶œëœ ì¢Œí‘œë“¤ë¡œë¶€í„° ê³„ì‚°ëœ ë³€í™˜ëœ ì¢Œí‘œë“¤
            if match.distance < self.max_dist * n.distance:
                pt1 = self.transformed_pts[match.queryIdx]  # self.kp1ì—ì„œ ë§¤ì¹­ëœ transformed_pts
                pt2 = self.kp2[match.trainIdx].pt         # self.kp2ì—ì„œ ë§¤ì¹­ëœ ì›ë³¸ ì´ë¯¸ì§€ì˜ ì¢Œí‘œ
                good_matches.append(match)
                # matched_ptsì— ë³€í™˜ëœ pt1ê³¼ ì›ë³¸ pt2 ì¢Œí‘œë¥¼ ì¶”ê°€
                object_points.append(pt1)
                image_points.append(pt2)
        # print(f'good_matches {good_matches}')
        if len(good_matches) < 4 :
            return
        self.result = True
        
        object_points = np.array(object_points)
        image_points = np.array(image_points)
        # print((object_points))
        # print()
        # print((image_points))
        # print(len(object_points))
        # print(len(image_points))
        # print(f'object_points : {object_points[:10]}')
        # print(f'image_points : {image_points[:10]}')
        # print(f'camera K : {self.CAMERA_K}')
        # print(f'camera K : {self.CAMERA_D}')
        initial_rotation = np.array([0, 0, 0], dtype=np.float64)  # íšŒì „ ë²¡í„°
        initial_translation = np.array([0, 0.3, 0.6], dtype=np.float64)  # ë³€í™˜ ë²¡í„° (zì¶•ìœ¼ë¡œ 0.35m)

        success, rvec, tvec, inliers = cv2.solvePnPRansac(object_points, image_points, self.CAMERA_K, self.CAMERA_D,
                                                        # useExtrinsicGuess=True, 
                                                        # rvec=initial_rotation, 
                                                        # tvec=initial_translation
                                                        )
        # success, rvec, tvec, inliers = cv2.solvePnPRansac(object_points, image_points, self.CAMERA_K, self.CAMERA_D,reprojectionError=2.0,iterationsCount=1000)
        if success:
            R, _ = cv2.Rodrigues(rvec)
            T = np.eye(4)
            T[:3, :3] = R
            T[:3, 3] = tvec.flatten()

            # print("âœ… Rotation Vector (rvec):\n", rvec)
            # print("âœ… Translation Vector (tvec):\n", tvec)
            # print("âœ… Transformation Matrix (T):\n", T)
            z = (T[:, 3].reshape(4, 1))
            # z[2] = z[2] * 2
            self.result_img =  z
            
            # ğŸ”¹ íŠ¹ì§•ì  ë§¤ì¹­ ì´ë¯¸ì§€ ìƒì„± ë° ì¶œë ¥
            # matched_image = cv2.drawMatches(self.ori_img, self.kp1, self.cap_img, self.kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
            # cv2.imshow("Feature Matching", matched_image)
            # cv2.waitKey(1000)
            # cv2.destroyAllWindows()
        self.cap_img = None
        
class ImageSubscriber(Node):
    def __init__(self, template_path_ext,template_path_man):
        super().__init__('image_subscriber')
        self.bridge = CvBridge()
        self.template_image_ext = cv2.imread(template_path_ext, cv2.IMREAD_GRAYSCALE)
        self.template_image_man = cv2.imread(template_path_man, cv2.IMREAD_GRAYSCALE)
        self.detector = None
        self.K = None
        self.D = None
        self.image_sub = self.create_subscription(
            CompressedImage, '/oakd/rgb/preview/image_raw/compressed', self.image_callback, 10)
        self.info_sub = self.create_subscription(CameraInfo, '/oakd/rgb/preview/camera_info', self.info_callback, 10)
        self.pose_sub = self.create_subscription(
            PoseWithCovarianceStamped,
            '/pose',
            self.pose_callback,
            10
        )
        # self.odom_sub = self.create_subscription(Odometry, '/odom', self.odom_callback, 10)
        self.publisher = self.create_publisher(Marker, 'visualization_marker', 10)
        self.timer = self.create_timer(1.0, self.publish_marker)  # 1ì´ˆë§ˆë‹¤ ì‹¤í–‰
        self.map_coords = None
        self.K = None  # ì¹´ë©”ë¼ ë‚´ì  í–‰ë ¬
        self.D = None  # ì™œê³¡ ê³„ìˆ˜
        self.tf_map_camera = None
        self.detector = SIFTDetector(self.template_image_ext, self.template_image_man, 1, self.K, self.D)

    def pose_callback(self, msg):
        # quaternion ê°’ì„ ë°›ì•„ì˜´
        # self.get_logger().info(f'ìŠ¤íƒ€í‹')
        translation = msg.pose.pose.position
        rotation = msg.pose.pose.orientation
        map_to_baselink = np.array([
                [rotation.x, rotation.y, rotation.z, translation.x],
                [rotation.y, rotation.x, rotation.z, translation.y],
                [rotation.z, rotation.z, rotation.x, translation.z],
                [0, 0, 0, 1]
            ])
        # ë³€í™˜ëœ íšŒì „ í–‰ë ¬ì„ ë¡œê·¸ë¡œ ì¶œë ¥
        quaternion = [rotation.x, rotation.y, rotation.z, rotation.w]
        # scipyì˜ Rotation í´ë˜ìŠ¤ë¥¼ ì´ìš©í•˜ì—¬ quaternionì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
        r = R.from_quat(quaternion)
        rotation_matrix_3x3 = r.as_matrix()
        map_to_baselink[:3, :3] = rotation_matrix_3x3 
        # self.get_logger().info(f'Rotation Matrix (4x4):\n{map_to_baselink}')
        self.map_coords = None
        self.tf_map_camera = np.dot(map_to_baselink, BASELINK_TO_CAMERA)

    def info_callback(self, msg):
        self.K = np.array(msg.k).reshape((3,3))
        self.D = np.array(msg.d).reshape((1,8))
        self.K[0][0] = 390
        self.K[1][1] = 390
        print(f"self.D {self.D} , self.K { self.K}")
        self.detector.set_KD(self.K,self.D)


    def image_callback(self, msg):
        try:
            self.get_logger().info(f"Received image with format: {msg.format}")
            cam_image = self.bridge.compressed_imgmsg_to_cv2(msg, "bgr8")
            cam_image = cv2.cvtColor(cam_image, cv2.COLOR_BGR2GRAY)
            self.detector.detect(cam_image)
            # print(f"man man ë¡œë´‡ ì¢Œí‘œê³„: {self.tf_map_camera}")
            # print(f"man man ë¡œë´‡ ê°ì§€: {self.detector.result}")
            print(f"man man ì¹´ë©”ë¼ë¡œë¶€í„° ì´ë¯¸ì§€ ê±°ë¦¬: \n {self.detector.result_img}")
            
            if self.detector.result and self.tf_map_camera is not None:
                # print("âœ… Object detected and mapped!")
                # print(f"TF Map to Camera: \n{self.tf_map_camera}")
                self.map_coords = np.dot(self.tf_map_camera, self.detector.result_img)
                if self.map_coords is not None:
                    pass
                    self.detector.result = False
                    # print(f"ğŸŒ Object in Map Coordinates: \n{self.map_coords}")
            else:
                print("âŒ No valid detection or missing TF data.")
        except Exception as e:
            self.get_logger().error(f"Error processing image: {e}")
            

    
    def publish_marker(self):
        marker = Marker()
        marker.header.frame_id = "map"  # RViz2ì˜ ê¸°ì¤€ ì¢Œí‘œê³„
        marker.header.stamp = self.get_clock().now().to_msg()
        marker.ns = "my_namespace"
        marker.id = 0
        marker.type = Marker.POINTS  # í¬ì¸íŠ¸ íƒ€ì… ë§ˆì»¤
        marker.action = Marker.ADD

        # ë§ˆì»¤ ìƒ‰ìƒ ë° í¬ê¸° ì„¤ì •
        marker.scale.x = 0.2  # í¬ì¸íŠ¸ í¬ê¸°
        marker.scale.y = 0.2
        marker.color.a = 1.0  # íˆ¬ëª…ë„
        marker.color.r = 0.0  # ë¹¨ê°„ìƒ‰
        marker.color.g = 1.0
        marker.color.b = 0.0
        if self.map_coords is None:
            return
        # ë§ˆì»¤ ìœ„ì¹˜ ì¶”ê°€
        point = Point()
        # point.x = self.map_coords[0][3]
        # point.y = self.map_coords[1][3]
        # point.z = self.map_coords[2][3]
        point.x = self.map_coords[0][0]
        point.y = self.map_coords[1][0]
        point.z = self.map_coords[2][0]
        marker.points.append(point)

        self.publisher.publish(marker)
        # self.get_logger().info(f'Published marker at {point.x} {point.y} {point.z}')

def main():
    rclpy.init()
    template_path_ext = "/home/sunwolee/Downloads/ext_orig.png"
    template_path_man = "/home/sunwolee/Downloads/man_orig.png"
    node = ImageSubscriber(template_path_ext,template_path_man)
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
